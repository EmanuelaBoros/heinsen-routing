{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of SST Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext as tt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pytorch_extras import RAdam, SingleCycleScheduler\n",
    "from pytorch_transformers import GPT2Model, GPT2Tokenizer\n",
    "from deps.torch_train_test_loop.torch_train_test_loop import LoopComponent, TrainTestLoop\n",
    "\n",
    "from heinsen_routing import Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained transformer and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained transformer loaded.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-large', do_lower_case=False)\n",
    "lang_model = GPT2Model.from_pretrained('gpt2-large', output_hidden_states=True, output_attentions=False)\n",
    "lang_model.cuda(device=DEVICE)\n",
    "lang_model.eval()\n",
    "print('Pretrained transformer loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_texts_to_embs(tokenized_texts, pad_token=tokenizer.eos_token):\n",
    "    tokenized_texts = [[*tok_seq, tokenizer.eos_token] for tok_seq in tokenized_texts]\n",
    "    lengths = [len(tok_seq) for tok_seq in tokenized_texts]\n",
    "\n",
    "    max_length = max(lengths)\n",
    "    input_toks = [t + [pad_token] * (max_length - l) for t, l in zip(tokenized_texts, lengths)]\n",
    "\n",
    "    input_ids = [tokenizer.convert_tokens_to_ids(tok_seq) for tok_seq in input_toks]\n",
    "    input_ids = torch.tensor(input_ids).to(device=DEVICE)\n",
    "\n",
    "    mask = [[1.0] * length + [0.0] * (max_length - length) for length in lengths]\n",
    "    mask = torch.tensor(mask).to(device=DEVICE)  # [batch sz, num toks]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = lang_model(input_ids=input_ids)\n",
    "        embs = torch.stack(outputs[-1], -2)  # [batch sz, n toks, n layers, d emb]\n",
    "\n",
    "    return mask, embs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_grained = False  # set to False for binary task\n",
    "train_on_subtrees = True  # set to False to train only on root sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets ready.\n",
      "Labels: (0, 'positive') (1, 'negative')\n",
      "Number of samples: 77,616 trn, 872 val, 1,821 tst.\n"
     ]
    }
   ],
   "source": [
    "tt.datasets.SST.download(root='.data')  # download if not present\n",
    "\n",
    "TEXT = tt.data.RawField(preprocessing=tokenizer.tokenize, postprocessing=tokenized_texts_to_embs, is_target=False)\n",
    "LABEL = tt.data.LabelField()\n",
    "\n",
    "class SSTFilter():\n",
    "\n",
    "    def __init__(self, remove_dupes=False, remove_neutral=False, min_n_toks=0):\n",
    "        self.remove_dupes, self.remove_neutral, self.min_n_toks = (remove_dupes, remove_neutral, min_n_toks)\n",
    "        self.prev_seen = {}\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        if self.remove_neutral and (sample.label == 'neutral'):\n",
    "            return False\n",
    "        if len(sample.text) < self.min_n_toks:\n",
    "            return False\n",
    "        hashable = ''.join(sample.text)\n",
    "        if self.remove_dupes and (hashable in self.prev_seen):\n",
    "            return False\n",
    "        self.prev_seen[hashable] = True\n",
    "        return True\n",
    "\n",
    "trn_ds = tt.datasets.SST(\n",
    "    '.data/sst/trees/train.txt', TEXT, LABEL, fine_grained=fine_grained, subtrees=train_on_subtrees,\n",
    "    filter_pred=SSTFilter(remove_neutral=False if fine_grained else True, remove_dupes=True))\n",
    "\n",
    "val_ds = tt.datasets.SST(\n",
    "    '.data/sst/trees/dev.txt', TEXT, LABEL, fine_grained=fine_grained, subtrees=False,\n",
    "    filter_pred=SSTFilter(remove_neutral=False if fine_grained else True, remove_dupes=False))\n",
    "\n",
    "tst_ds = tt.datasets.SST(\n",
    "    '.data/sst/trees/test.txt', TEXT, LABEL, fine_grained=fine_grained, subtrees=False,\n",
    "    filter_pred=SSTFilter(remove_neutral=False if fine_grained else True, remove_dupes=False))\n",
    "\n",
    "LABEL.build_vocab(trn_ds)\n",
    "\n",
    "print('Datasets ready.\\nLabels:', *enumerate(LABEL.vocab.stoi))\n",
    "print('Number of samples: {:,} trn, {:,} val, {:,} tst.'.format(len(trn_ds), len(val_ds), len(tst_ds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceRoutingClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, d_depth, d_emb, d_caps, n_caps, **kwargs):\n",
    "        super().__init__()\n",
    "        zipped_cap_args = zip(d_caps[1:], n_caps[1:], d_caps[:-1], n_caps[:-1])\n",
    "        self.depth_emb = nn.Parameter(torch.zeros(d_depth, d_emb))\n",
    "        self.routings = nn.Sequential(\n",
    "            Routing(1, d_out=d_caps[0], n_out=n_caps[0], d_inp=d_emb, **kwargs),\n",
    "            *[Routing(1, *cap_args, **kwargs) for cap_args in zipped_cap_args],\n",
    "        )\n",
    "\n",
    "    def forward(self, mask, embs):\n",
    "        embs = embs + self.depth_emb  # [bs, n toks, d_depth, d_emb]\n",
    "\n",
    "        a = torch.log(mask / (1.0 - mask))  # -inf to inf (PyTorch handles this nicely)\n",
    "        a = a.unsqueeze(-1).expand(-1, -1, embs.shape[-2]).contiguous()  # [bs, n toks, d_depth]\n",
    "        a = a.view(a.shape[0], -1)  # [bs, n toks * d_depth]\n",
    "        mu = embs.view(embs.shape[0], -1, 1, embs.shape[-1])  # [bs, n toks * d_depth, 1, d_depth]\n",
    "\n",
    "        for routing in self.routings:\n",
    "            a, mu, sig2 = routing(a, mu)\n",
    "\n",
    "        return a, mu, sig2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoopMain(LoopComponent):\n",
    "\n",
    "    def __init__(self, n_classes, device, min_prob=0.0, pct_warmup=0.1, mixup=(0.2, 0.2)):\n",
    "        self.n_classes, self.device, self.min_prob, self.pct_warmup = (n_classes, device, min_prob, pct_warmup)\n",
    "        self.mixup_dist = torch.distributions.Beta(torch.tensor(mixup[0]), torch.tensor(mixup[1]))\n",
    "\n",
    "        self.onehot = torch.eye(self.n_classes, device=self.device)\n",
    "        self.smooth = (self.onehot * (1.0 - (self.min_prob * self.n_classes))) + self.min_prob\n",
    "\n",
    "    def on_train_begin(self, loop):\n",
    "        n_iters = len(loop.train_data) * loop.n_epochs\n",
    "        loop.optimizer = RAdam([{ 'params': loop.model.parameters(), 'lr': 5e-4 }])\n",
    "        loop.scheduler = SingleCycleScheduler(loop.optimizer, loop.n_optim_steps, frac=self.pct_warmup, min_lr=1e-5)\n",
    "        \n",
    "    def on_grads_reset(self, loop):\n",
    "        loop.model.zero_grad()\n",
    "\n",
    "    def on_forward_pass(self, loop):\n",
    "        model, batch = (loop.model, loop.batch)\n",
    "        mask, embs = batch.text\n",
    "\n",
    "        if loop.is_training:\n",
    "            r = self.mixup_dist.sample([len(mask)]).to(device=mask.device)\n",
    "            idx = torch.randperm(len(mask))\n",
    "            mask = mask.lerp(mask[idx], r[:, None])\n",
    "            embs = embs.lerp(embs[idx], r[:, None, None, None])\n",
    "            target_probs = self.smooth[batch.label]\n",
    "            target_probs = target_probs.lerp(target_probs[idx], r[:, None])\n",
    "        else:\n",
    "            target_probs = self.onehot[batch.label]\n",
    "\n",
    "        # Classify inputs.\n",
    "        pred_scores, _, _ = model(mask, embs)\n",
    "        _, pred_ids = pred_scores.max(-1)\n",
    "        accuracy = (pred_ids == batch.label).float().mean()\n",
    "\n",
    "        # Save results as loop attrs.\n",
    "        loop.pred_scores, loop.target_probs, loop.accuracy = (pred_scores, target_probs, accuracy)\n",
    "\n",
    "    def on_loss_compute(self, loop):\n",
    "        losses = -loop.target_probs * F.log_softmax(loop.pred_scores, dim=-1)  # smooth cross entropy\n",
    "        loop.loss = losses.sum(dim=-1).mean()  # sum across classes, then mean of batch\n",
    "\n",
    "    def on_backward_pass(self, loop):\n",
    "        loop.loss.backward()\n",
    "\n",
    "    def on_optim_step(self, loop):\n",
    "        loop.optimizer.step()\n",
    "        loop.scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoopStats(LoopComponent):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "\n",
    "    def on_batch_end(self, loop):\n",
    "        self.data.append({\n",
    "            'n_samples': len(loop.batch),\n",
    "            'n_toks': loop.batch.text[1].shape[1],\n",
    "            'epoch_desc': loop.epoch_desc,\n",
    "            'epoch_num': loop.epoch_num,\n",
    "            'epoch_frac': loop.epoch_num + loop.batch_num / loop.n_batches,\n",
    "            'accuracy': loop.accuracy.item(),\n",
    "            'loss': loop.loss.item(),\n",
    "            'lr': loop.optimizer.param_groups[0]['lr'],\n",
    "            'momentum': loop.optimizer.param_groups[0]['betas'][0],\n",
    "        })\n",
    "\n",
    "    def plot(self, item_name='loss', epoch_desc='train', groupby='epoch_frac', **kwargs):\n",
    "        df = pd.DataFrame(self.data)\n",
    "        df = df[df.epoch_desc == epoch_desc]\n",
    "        df[item_name] = df[item_name] * df.n_samples\n",
    "        series = df.groupby(groupby)[item_name].sum() / df.groupby(groupby).n_samples.sum()\n",
    "        series.plot(label=f\"{epoch_desc}_{item_name}\", **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoopProgressBar(LoopComponent):\n",
    "\n",
    "    def __init__(self, item_names=['loss', 'accuracy'], show_train_stats=False):\n",
    "        self.item_names, self.show_train_stats = (item_names, show_train_stats)\n",
    "\n",
    "    def on_epoch_begin(self, loop):\n",
    "        self.total, self.count = ({ name: 0.0 for name in self.item_names }, 0)\n",
    "        self.pbar = tqdm(total=loop.n_batches, desc=f\"{loop.epoch_desc} epoch {loop.epoch_num}\")\n",
    "\n",
    "    def on_batch_end(self, loop):\n",
    "        n = len(loop.batch)\n",
    "        self.count += n\n",
    "        for name in self.item_names:\n",
    "            self.total[name] += getattr(loop, name).item() * n\n",
    "        self.pbar.update(1)\n",
    "\n",
    "        if (not loop.is_training) or (loop.is_training and self.show_train_stats):\n",
    "            self.pbar.set_postfix(self.mean)\n",
    "\n",
    "    def on_epoch_end(self, loop):\n",
    "        self.pbar.close()\n",
    "\n",
    "    def on_train_end(self, loop):\n",
    "        self.pbar.close()  # in case of early stop\n",
    "\n",
    "    @property\n",
    "    def mean(self): return { f'mean_{name}': self.total[name] / self.count for name in self.item_names }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 245,248.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Seed RNG for replicability.\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Make iterators for each split, with random shuffling.\n",
    "trn_itr, val_itr, tst_itr = tt.data.BucketIterator.splits(\n",
    "    (trn_ds, val_ds, tst_ds),\n",
    "    shuffle=True,\n",
    "    batch_size=4,\n",
    "    device='cuda:0')\n",
    "\n",
    "# Initialize model.\n",
    "model = SequenceRoutingClassifier(\n",
    "    d_depth=lang_model.config.n_layer + 1,\n",
    "    d_emb=lang_model.config.hidden_size,\n",
    "    d_caps=[2, 2, 2],\n",
    "    n_caps=[64, 64, len(LABEL.vocab)],\n",
    ")\n",
    "model = model.cuda(device=DEVICE)\n",
    "\n",
    "print('Total number of parameters: {:,}.\\n'.format(sum(int(np.prod(p.shape)) for p in model.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch 0: 100%|██████████| 19404/19404 [45:47<00:00,  7.06it/s, mean_loss=0.427, mean_accuracy=0.743] \n",
      "valid epoch 0: 100%|██████████| 218/218 [00:20<00:00, 10.74it/s, mean_loss=0.229, mean_accuracy=0.914]\n",
      "train epoch 1: 100%|██████████| 19404/19404 [45:55<00:00,  7.04it/s, mean_loss=0.367, mean_accuracy=0.768] \n",
      "valid epoch 1: 100%|██████████| 218/218 [00:20<00:00, 10.71it/s, mean_loss=0.2, mean_accuracy=0.924]  \n",
      "train epoch 2: 100%|██████████| 19404/19404 [46:01<00:00,  7.03it/s, mean_loss=0.343, mean_accuracy=0.776] \n",
      "valid epoch 2: 100%|██████████| 218/218 [00:20<00:00, 10.60it/s, mean_loss=0.198, mean_accuracy=0.922]\n",
      "train epoch 3:   2%|▏         | 462/19404 [01:04<43:17,  7.29it/s, mean_loss=0.317, mean_accuracy=0.79]   "
     ]
    }
   ],
   "source": [
    "loop_components = [\n",
    "    LoopMain(len(LABEL.vocab), DEVICE, pct_warmup=0.1, mixup=(0.8, 0.8)),\n",
    "    LoopStats(),\n",
    "    LoopProgressBar(show_train_stats=True),\n",
    "]\n",
    "loop = TrainTestLoop(model, loop_components, trn_itr, val_itr)\n",
    "loop.train(n_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop.components[-1].pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize training stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(16, 5))\n",
    "stats = loop.components[-2]\n",
    "for axis, item_name in zip(axes, ['loss', 'accuracy']):\n",
    "    #stats.plot(item_name, 'train', ax=axis, legend=True, alpha=0.5, color='red')\n",
    "    stats.plot(item_name, 'valid', ax=axis, legend=True, alpha=0.5, color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(16, 5))\n",
    "stats = loop.components[-2]\n",
    "for axis, item_name in zip(axes, ['loss', 'accuracy']):\n",
    "    stats.plot(item_name, 'train', groupby='epoch_num', ax=axis, legend=True, alpha=0.5, color='red')\n",
    "    stats.plot(item_name, 'valid', groupby='epoch_num', ax=axis, legend=True, alpha=0.5, color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop.test(tst_itr)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "s = 'SST2_pretrained_model'\n",
    "pd.DataFrame(loop.components[1].data).to_csv(f'{s}_TRAINING_DATA.csv', index=False)\n",
    "torch.save(model.state_dict(), f'{s}_MODEL_STATE.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
